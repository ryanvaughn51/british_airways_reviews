{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39e1d668-2687-40d2-906f-5df13de049e5",
   "metadata": {},
   "source": [
    "# British Airways Web Scraping of Skytrax Review Site\n",
    "### Web Scrape, Data Cleaning, and Data Verification\n",
    "\n",
    "[Skytrax](https://www.airlinequality.com/) is a website where travellers can check reviews for airlines and make reviews themselves. There are reviews for all major airlines.\n",
    "\n",
    "Along with a title for their review and a paragraph describing their experience --\n",
    "\n",
    "Reviewers gave data on the following:\n",
    "\n",
    "- Aircraft Type\n",
    "- Type of Traveller (solo, business, etc)\n",
    "- Seat Type\n",
    "- Route\n",
    "- Date of Flight\n",
    "\n",
    "And reviewers also gave a star rating (from 1 to 5) for the following categories:\n",
    "\n",
    "- Seat Comfort\n",
    "- Cabin Staff Service\n",
    "- Food & Beverages\n",
    "- Inflight Entertainment\n",
    "- Ground Service\n",
    "- Wifi & Connectivity\n",
    "- Value for the Money\n",
    "\n",
    "Reviewers also gave an overall rating (from 1 to 10).\n",
    "\n",
    "Lastly, reviewers recommended the airline (Yes/No).\n",
    "\n",
    "This project will scrape all of this data from the skytrax website and put in tabular form. Then, the data can be analyzed for insights that can help British Airways.\n",
    "\n",
    "Here, we will scrape the web, clean the data, and output two .csv files with the relevant data that can be used by anyone interested in analysing the data.\n",
    "\n",
    "Python will be used for this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adabb1da-add1-47f4-926b-0ef8781a4b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a135318-4418-4adf-9a21-101b1c125f5f",
   "metadata": {},
   "source": [
    "## Web Scraping\n",
    "\n",
    "At the time of this writing, there were 3426 British Airways reviews on skytrax from 2011 to 2022. The reviews were listed in pagesizes of 100 reviews. After creating a list of all of the 35 urls for each of the 100 reviews, we'll iterate through them and scrape all necessary information into their respective lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f879613-e896-4f11-851f-9371a3b87117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of urls for the various scroll pages of results\n",
    "urls = []\n",
    "for i in range(35):\n",
    "    page = i+1\n",
    "    urls.append(f'https://www.airlinequality.com/airline-reviews/british-airways/page/{page}/?sortby=post_date%3ADesc&pagesize=100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "1c9da42b-d76c-4f22-9d21-d37aad38254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will scrape all necessary information from the skytrax review pages and store in the following lists\n",
    "# We use the python BeautifulSoup package\n",
    "\n",
    "title = []\n",
    "star_fill = []\n",
    "star_fill_page = []\n",
    "review_value = []\n",
    "reviews = []\n",
    "rating_header = []\n",
    "rating_page = []\n",
    "rating = []\n",
    "\n",
    "data_string = \"\"\n",
    "\n",
    "for url in urls: \n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    \n",
    "    # Title\n",
    "    for item in soup.find_all(\"h2\", class_=\"text_header\"):\n",
    "        data_string = data_string + item.get_text()\n",
    "        title.append(data_string)\n",
    "        data_string = \"\"\n",
    "    \n",
    "    # Overall Rating\n",
    "    for item in soup.find_all(\"span\", itemprop=\"ratingValue\"):\n",
    "        data_string = data_string + item.get_text()\n",
    "        rating_page.append(data_string)\n",
    "        data_string = \"\"\n",
    "    rating_page = rating_page[1:]\n",
    "    for i in rating_page:\n",
    "        rating.append(i)\n",
    "    rating_page = []\n",
    "\n",
    "    # Star Ratings\n",
    "    for item in soup.find_all(\"span\", class_=\"star fill\"):\n",
    "        data_string = data_string + item.get_text()\n",
    "        star_fill_page.append(data_string)\n",
    "        data_string = \"\"\n",
    "    star_fill_page = star_fill_page[15:]\n",
    "    for i in star_fill_page:\n",
    "        star_fill.append(i)\n",
    "    star_fill_page = []\n",
    "        \n",
    "    # Aircraft, Traveller Type, Seat Type, Route, Date Flown, Recommended    \n",
    "    for item in soup.find_all(\"td\", class_=\"review-value\"):\n",
    "        data_string = data_string + item.get_text()\n",
    "        review_value.append(data_string)\n",
    "        data_string = \"\"\n",
    "        \n",
    "    # Rating Headers\n",
    "    for item in soup.find_all(\"td\", class_=\"review-rating-header\"):\n",
    "        data_string = data_string + item.get_text()\n",
    "        rating_header.append(data_string)\n",
    "        data_string = \"\"\n",
    "\n",
    "    # Review Body, Verification    \n",
    "    for item in soup.find_all(\"div\", class_=\"text_content\"):\n",
    "        data_string = data_string + item.get_text()\n",
    "        reviews.append(data_string)\n",
    "        data_string = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9211af8e-4f74-4a88-a583-775d4c6c66ba",
   "metadata": {},
   "source": [
    "## Data Cleaning and Data Frame Creation\n",
    "\n",
    "Without going into exhaustive detail about the quirks of how the data is displayed on skytrax's website (which is all perfectly reasonable for skytrax to display it this way...it just makes the web scraping more problematic), we'll summarize a few interesting details that will explain some of the following code:\n",
    "\n",
    "Some details:\n",
    "- At the top of each page of 100 results is a summary of five metrics. This information was scraped along with everything else and had to be deleted every 100 entries.\n",
    "- At some point, in 2015, how the reviews were displayed changed. Rules of what reviewers were required to input changed. Not giving a rating for a certain aspect of the flight is now handled by skytrax by leaving the category out, but before is sometimes (not always!) expressed with a 'NA'.\n",
    "\n",
    "Another difficulty is how the information was scraped. One result list had all of the reading headers (i.e. the column names in a data frame). But not every review had data for each header. Another list had the star values--just the numbers--and another list had the airplane type, seat type, etc, data. \n",
    "\n",
    "Our main strategy to create a usable data set is to first make an empty data frame filled with na values and then iterate through our header list. For each item, find the appropriate value from either the stars list or the review value list to fill up the data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef910ca-36f4-41f1-97e8-d323e78e7523",
   "metadata": {},
   "source": [
    "### Rating Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "04cd50c7-c7f6-40ea-8ee3-b1a3602c400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the first five summary ratings on the first page\n",
    "rating_header_t = rating_header[5:]\n",
    "rating_header_df = pd.DataFrame(dict(rating_header=rating_header_t))\n",
    "rating_header_df['rating_header'] = rating_header_df.rating_header.str.replace('Cabin Staff Service','Staff Service').str.lower()\n",
    "\n",
    "# remove the five summary ratings from all of the other results pages\n",
    "# here we're using a fortunate result that every review has a recommendation\n",
    "index = 0\n",
    "count = 0\n",
    "for i in rating_header_df.rating_header:\n",
    "    if count == 100:\n",
    "        rating_header_df.drop(range(index,index+5), axis=0, inplace=True)\n",
    "        count = 0\n",
    "    else:\n",
    "        if i == 'recommended':\n",
    "            count += 1\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c67d205-d4ce-4258-9cc8-b4a4f703b4f4",
   "metadata": {},
   "source": [
    "### Star Count\n",
    "\n",
    "A quirk here is that the scraping picks up every star, not the highest star value for each rating. We delete the unnecessary stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "e246d396-8190-4d52-b9fa-d8049692bd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stars = []\n",
    "for i in range(len(star_fill)-1):\n",
    "    if star_fill[i+1] == '1':\n",
    "        stars.append(star_fill[i])\n",
    "# create an iterator from the star ratings\n",
    "stars_iterator = iter(stars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afda7034-6ec5-4be1-9fba-0a32338a7b19",
   "metadata": {},
   "source": [
    "### Review Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "dd1a2aea-2d6f-4ce2-8d05-0d48fadf0436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an iterator from the review values\n",
    "review_value_iterator = iter([i.lower() for i in review_value])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e36fb42-b2f2-4720-8134-c8c9d46aae01",
   "metadata": {},
   "source": [
    "## Create Information and Ratings DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "16f5965b-43ea-4ceb-aa5b-df3643fc9142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 3426 total reviews in our data\n",
    "# The header order below reflects the order present on the skytrax website\n",
    "df_headers = pd.DataFrame(index=range(3426))\n",
    "header_order = ['aircraft','type of traveller','seat type','route','date flown','seat comfort',\n",
    "                'staff service','food & beverages','inflight entertainment','ground service',\n",
    "                'wifi & connectivity','value for money','recommended']\n",
    "# Create a dataframe with 3426 rows (above) and the header order for columns. \n",
    "# Filled with na for now.\n",
    "for header in header_order:\n",
    "    df_headers[header]=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "115f4bff-385b-4708-99b5-614740faeb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the headers \n",
    "\n",
    "row = 0\n",
    "for item in rating_header_df.rating_header:\n",
    "    try:\n",
    "        if item == 'recommended':\n",
    "            df_headers.loc[row, item] = next(review_value_iterator)\n",
    "            row += 1\n",
    "            continue\n",
    "        elif item in ['aircraft','type of traveller','seat type','route','date flown']:\n",
    "            df_headers.loc[row, item] = next(review_value_iterator)\n",
    "        else:\n",
    "            df_headers.loc[row, item] = next(stars_iterator)\n",
    "    # the really old data from skytrax was too inconsistent with their stars, and will eventually \n",
    "    # give an error. We'll avoid that here knowing we'll drop the oldest data points anyway.\n",
    "    except StopIteration:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "f831d857-1020-41d0-b7f1-c2172abc2e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No missing data for the title or reviews. Straight forward to create the data frame and\n",
    "# add it to the data frame above.\n",
    "\n",
    "df = pd.DataFrame(dict(title=title, reviews=reviews))\n",
    "\n",
    "reviews_only = []\n",
    "for i in df.reviews:\n",
    "    if '|' in i:\n",
    "        reviews_only.append(i.split('|')[1])\n",
    "    else:\n",
    "        reviews_only.append(i)\n",
    "        \n",
    "df['reviews'] = reviews_only\n",
    "    \n",
    "df['reviews'] = df.reviews.str.strip() \\\n",
    "                          .str.lower()\n",
    "df['title'] = df.title.str.strip('\"') \\\n",
    "                      .str.lower()\n",
    "\n",
    "ba_data = pd.concat([df_headers, df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "a60d244d-7de1-4ce1-9712-96ea692113ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the user's overall rating \n",
    "ba_data.drop(ba_data.tail(6).index, axis=0, inplace=True)\n",
    "ba_data['rating'] = rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60191589-18b4-4a0c-8370-b066db1c7287",
   "metadata": {},
   "source": [
    "Skytrax apparently changed how they take reviews, changing the format, what's required for reviewers to enter, etc. After creating the above dataset, a random sequential visual sampling of the skytrax website vs the dataset, and the first 2649 data points were verified to be correctly matching the information on the website. After that, the format changed and the information on our data frame was incorrect. Our final step is to drop those roughly 800 rows. While this is not ideal, we are lucky that the oldest data is what we have to drop.\n",
    "\n",
    "We will, in a separate data set, keep all of the written reviews from all the way back to 2011."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "1003b1e1-18cc-42a1-928f-b9025c87ffeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only verified rows of data\n",
    "ba_data_verified = ba_data.loc[:2648, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "2895ba78-8e72-46d6-81de-c80797973111",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/91/f5k555x942n96pyy4gdm3ymh0000gn/T/ipykernel_1368/3296671848.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ba_data_verified['date_dt'] = pd.to_datetime(ba_data_verified['date flown'], format='%B %Y')\n",
      "/var/folders/91/f5k555x942n96pyy4gdm3ymh0000gn/T/ipykernel_1368/3296671848.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ba_data_verified.drop('date flown', axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# final cleaning steps\n",
    "\n",
    "# create datetime column and drop original date column\n",
    "ba_data_verified['date_dt'] = pd.to_datetime(ba_data_verified['date flown'], format='%B %Y')\n",
    "ba_data_verified.drop('date flown', axis=1, inplace=True)\n",
    "\n",
    "# clean up column names\n",
    "ba_data_verified.columns = ba_data_verified.columns.str.replace(' ','_').str.replace('&','and')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "6230094d-beb0-4127-92b2-175a36aa097d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all data to .csv\n",
    "ba_data_verified.to_csv('british_airways_skytrax_2015-2022_review_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
